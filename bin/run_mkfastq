#!/usr/bin/env bash

# This script is the entrypoint for all invocations of `cellranger
# mkfastq`. It pulls the raw data from s3 and generates
# samplesheet.csv before invoking `cellranger mkfastq`.

set -euo pipefail

if [[ "${DEBUG-false}" == "true" ]]; then
  set -x
fi

# SCRATCH_DIR is a 1TB volume.
SCRATCH_DIR="$HOME/scratch"
COMPRESSED_RAW_DATA_PATH="${SCRATCH_DIR}/raw_data.tar.gz"
OUTPUT_DIR="${SCRATCH_DIR}/output"
RAW_DATA_DIR="${SCRATCH_DIR}/raw_data"
SAMPLESHEET_CSV_PATH="${SCRATCH_DIR}/samplesheet.csv"
S3_BUCKET="s3://10x-data-backup"

msg() {
  echo "$@" >&2
}

usage() {
  echo "$0 CONFIG_JSON"
}

get_from_config() {
  local key
  key="$1"
  python -c "import json; import sys;
print(json.load(sys.stdin)['${key}'])"
}

fetch_raw_data() {
  local config_json experiment_name bcl_filename
  config_json="$1"
  experiment_name="$(get_from_config experiment_name <<<"$config_json")"
  bcl_file="$(get_from_config bcl_file <<<"$config_json")"
  mkdir -p "$RAW_DATA_DIR"
  aws s3 cp "${S3_BUCKET}/${experiment_name}/raw_data/${bcl_file}" - \
    | tar -xvz -C "$RAW_DATA_DIR" --strip 1 -f -
}

cellranger_mkfastq() {
  local stderr_log raw_data_dir

  # Because we get bcl files from different sources, RAW_DATA_DIR may have a different
  # structure. Ultimately, we are looking for the directory with a floatin RTAComplete.txt
  # file.

  if [ -e "${RAW_DATA_DIR}/RTAComplete.txt" ]; then
    echo "RTAComplete.txt file found"
    raw_data_dir="$RAW_DATA_DIR"
  else
    # update raw_data_dir
    raw_data_dir="$(dirname "$(find "${RAW_DATA_DIR}" -name "RTAComplete.txt")")"
  fi

  # the subshell allows us to properly scope this trap.
  (
    # think of this as a try/catch or begin/rescue'.
    trap log_mkfastq_error err
    cellranger mkfastq \
               --disable-ui \
               --delete-undetermined \
               --run="${raw_data_dir}" \
               --output-dir="${OUTPUT_DIR}" \
               --samplesheet="${SAMPLESHEET_CSV_PATH}" \
      | tee mkfastq.log
  )
}

log_mkfastq_error() {
  # Cellranger logs to disk - we need to read the error log from disk
  # before the container exits and is gone for good, as otherwise the
  # error will be lost.

  if grep -E '_stderr$' mkfastq.log; then
    # Assume that it's telling us where to read error logs.
    stderr_log="$(grep -E '_stderr$' mkfastq.log)"
    msg "reading error log $stderr_log:"
    cat "$stderr_log"
  else
    msg "cellranger_mkfastq: error. Attempting to read error log."
    find "$HOME" -name _errors
    find "$HOME" -name _errors | xargs head -n99999
  fi

  exit 1
}

sample_names() {
  python -c "import json; import sys;
for sample in json.load(sys.stdin)['samples']:
  print(sample['name'])"
}

upload_results() {
  local config_json experiment_name run_id
  config_json="$1"
  experiment_name="$(get_from_config experiment_name <<<"$config_json")"
  run_id="$(get_from_config run_id <<<"$config_json")"

  for sample_name in $(sample_names <<<"$config_json"); do
    for fastq in ${OUTPUT_DIR}/${sample_name}_S*fastq.gz; do
      aws s3 cp \
          "$fastq" \
          "${S3_BUCKET}/${experiment_name}/${sample_name}/fastqs/run${run_id}/$(basename "$fastq")"
    done
  done

  for dir in Reports Stats; do
    aws s3 cp \
        ${OUTPUT_DIR}/${dir} \
        ${S3_BUCKET}/${experiment_name}/fastqs_metadata/gex/run${run_id}/${dir} \
        --recursive
  done
}

main() {
  local config_json

  if [[ -z "${1:-}" ]]; then
    usage
    exit 1
  fi

  config_json="$1"

  fetch_raw_data "$config_json"
  generate_samplesheet_mkfastq "$config_json" > "$SAMPLESHEET_CSV_PATH"
  cellranger_mkfastq "$config_json"
  upload_results "$config_json"
}

main "$@"
