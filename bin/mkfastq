#!/usr/bin/env bash

# This script is the entrypoint for all invocations of `cellranger
# mkfastq`. It pulls the raw data from s3 and generates
# samplesheet.csv before invoking `cellranger mkfastq`.

set -euo pipefail

if [[ "${DEBUG-false}" == "true" ]]; then
  set -x
fi

COMPRESSED_RAW_DATA_PATH="/tmp/raw_data.tar.gz"
OUTPUT_DIR="${HOME}/output"
RAW_DATA_DIR="${HOME}/raw_data"
SAMPLESHEET_CSV_PATH="${HOME}/samplesheet.csv"
S3_BUCKET="s3://10x-data-backup"

msg() {
  echo "$@" >&2
}

usage() {
  echo "$0 CONFIG_JSON"
}

get_from_config() {
  local key
  key="$1"
  python -c "import json; import sys;
print(json.load(sys.stdin)['${key}'])"
}

fetch_raw_data() {
  local config_json experiment_name bcl_filename
  config_json="$1"
  experiment_name="$(get_from_config experiment_name <<<"$config_json")"
  bcl_file="$(get_from_config bcl_file <<<"$config_json")"
  aws s3 cp "${S3_BUCKET}/${experiment_name}/raw_data/${bcl_file}" "$COMPRESSED_RAW_DATA_PATH"
  mkdir -p "$RAW_DATA_DIR"
  tar -xvz -C "$RAW_DATA_DIR" --strip 1 -f "$COMPRESSED_RAW_DATA_PATH"
  rm "$COMPRESSED_RAW_DATA_PATH"
}

cellranger_mkfastq() {
  # TOOD: find out if we really need to pass these when running in a
  # container. hoping that it defaults to no maxes
  #
  # --localcores="${LOCALCORES}" \
  # --localmem="${LOCALMEM}" \
  local stderr_log

  # the subshell allows us to properly scope this trap.
  (
    trap mkfastq_error_handler err
    cellranger mkfastq \
               --delete-undetermined \
               --run="${RAW_DATA_DIR}" \
               --output-dir="${OUTPUT_DIR}" \
               --samplesheet="${SAMPLESHEET_CSV_PATH}" \
      | tee mkfastq.log
  )
}

mkfastq_error_handler() {
  if grep -E '_stderr$' mkfastq.log; then
    # Assume that it's telling us where to read error logs.
    stderr_log="$(grep -E '_stderr$' mkfastq.log)"
    msg "reading error log $stderr_log:"
    cat "$stderr_log"
  else
    msg "cellranger_mkfastq: error. Attempting to read error log."
    find "$HOME" -name _errors
    find "$HOME" -name _errors | xargs head -n99999
  fi

  exit 1
}

sample_names() {
  python -c "import json; import sys;
for sample in json.load(sys.stdin)['samples']:
  print(sample['name'])"
}

upload_results() {
  local config_json experiment_name fastqs
  config_json="$1"
  experiment_name="$(get_from_config experiment_name <<<"$config_json")"

  for sample_name in $(sample_names <<<"$config_json"); do
    for fastq in ${OUTPUT_DIR}/${sample_name}_S*fastq.gz; do
      aws s3 cp "$fastq" "${S3_BUCKET}/${experiment_name}/${sample_name}/"
    done
  done

  for dir in Reports Stats; do
    aws s3 cp --recursive ${OUTPUT_DIR}/${dir} ${S3_BUCKET}/${experiment_name}/fastqs/gex/${dir}
  done
}

main() {
  local config_json

  if [[ -z "${1:-}" ]]; then
    usage
    exit 1
  fi

  config_json="$1"

  fetch_raw_data "$config_json"
  generate_samplesheet "$config_json" > "$SAMPLESHEET_CSV_PATH"
  cellranger_mkfastq "$config_json"
  upload_results "$config_json"
}

main "$@"
