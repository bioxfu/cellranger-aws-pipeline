#!/usr/bin/env bash

# This script is the entrypoint for all invocations of `cellranger
# mkfastq`. It pulls the raw data from s3 and generates
# samplesheet.csv before invoking `cellranger mkfastq`.

set -euo pipefail

if [[ "${DEBUG-false}" == "true" ]]; then
  set -x
fi

COMPRESSED_RAW_DATA_PATH="/tmp/raw_data.tar.gz"
LOCALMEM=124 # TODO: don't hardcode
LOCALCORES=15 # TODO: don't hardcode
OUTPUT_DIR="${HOME}/output"
RAW_DATA_DIR="${HOME}/raw_data"
SAMPLESHEET_CSV_PATH="${HOME}/samplesheet.csv"

msg() {
  echo "$@" >&2
}

usage() {
  echo "$0 CONFIG_JSON"
}

get_from_config() {
  local key
  key="$1"
  python -c "import json; import sys;
print(json.load(sys.stdin)['${key}'])"
}

fetch_raw_data() {
  local config_json experiment_name bcl_filename
  config_json="$1"
  experiment_name="$(get_from_config experiment_name <<<"$config_json")"
  bcl_file="$(get_from_config bcl_file <<<"$config_json")"
  aws s3 cp "s3://10x-data-backup/${experiment_name}/raw_data/${bcl_file}" "$COMPRESSED_RAW_DATA_PATH"
  mkdir -p "$RAW_DATA_DIR"
  tar -xvz -C "$RAW_DATA_DIR" --strip 1 -f "$COMPRESSED_RAW_DATA_PATH"
  rm "$COMPRESSED_RAW_DATA_PATH"
}

cellranger_mkfastq() {
  # TOOD: find out if we really need to pass these when running in a
  # container. hoping that it defaults to no maxes
  #
  # --localcores="${LOCALCORES}" \
  # --localmem="${LOCALMEM}" \
  local stderr_log

  # the subshell allows us to properly scope this trap.
  (
    trap mkfastq_error_handler err
    cellranger mkfastq \
               --delete-undetermined \
               --run="${RAW_DATA_DIR}" \
               --output-dir="${OUTPUT_DIR}" \
               --samplesheet="${SAMPLESHEET_CSV_PATH}" \
      | tee mkfastq.log
  )
}

mkfastq_error_handler() {
  if grep -E '_stderr$' mkfastq.log; then
    # Assume that it's telling us where to read error logs.
    stderr_log="$(grep -E '_stderr$' mkfastq.log)"
    msg "reading error log $stderr_log:"
    cat "$stderr_log"
  else
    msg "cellranger_mkfastq: error. Attempting to read error log."
    find "$HOME" -name _errors
    find "$HOME" -name _errors | xargs head -n99999
  fi

  exit 1
}

upload_results() {
  # FIXME:
  local sample_id experiment_name
  experiment_name="$(get_from_config experiment_name <<<"$config_json")"
  ls "$OUTPUT_DIR"
  # output:
  # Reports
  # Stats
  # test_sample_S1_L001_I1_001.fastq.gz
  # test_sample_S1_L001_R1_001.fastq.gz
  # test_sample_S1_L001_R2_001.fastq.gz
  for fastq in "$OUTPUT_DIR"/*; do
    aws s3 cp "$fastq" s3://10x-data-backup/${experiment_name}/${sample_id}/
  done
}

main() {
  local config_json

  if [[ -z "${1:-}" ]]; then
    usage
    exit 1
  fi

  config_json="$1"

  fetch_raw_data "$config_json"
  generate_samplesheet "$config_json" > "$SAMPLESHEET_CSV_PATH"
  cellranger_mkfastq "$config_json"
  upload_results
}

main "$@"
