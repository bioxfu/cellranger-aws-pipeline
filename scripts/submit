#!/usr/bin/env python3.6

# This script submits experiment jobs to AWS Batch.

import boto3
import datetime as dt
import json
from jsonschema import Draft4Validator, RefResolver
import os
import sys
import yaml

AWS_ECR_REGISTRY = '402084680610.dkr.ecr.us-east-1.amazonaws.com'

CELLRANGER_PIPELINE_ENV = os.environ.get('CELLRANGER_PIPELINE_ENV', 'prod')
PIPELINE_BASE_NAME = f'{CELLRANGER_PIPELINE_ENV}-cellranger-pipeline'
JOB_QUEUE = PIPELINE_BASE_NAME
SEQUENCING_RUN_NAME_DELIMITER = '-'
SEQUENCING_RUN_FIELD_DELIMITER = '_'
S3_BUCKET='10x-data-backup'

batch_client = boto3.client('batch')
s3_client = boto3.client('s3')

def usage():
  print(f"""
usage: {sys.argv[0]} CONFIG_YAML_FILENAME

CONFIG_YAML_FILENAME: path to config yaml relative to the user's pwd.
""")

def generate_sequencing_run_name(sequencing_run):
  run_id = sequencing_run['id']
  himc_pool = sequencing_run['himc_pool']
  sequencing_date = sequencing_run['date']
  sequencing_date_object = dt.datetime.strptime(sequencing_date, "%Y-%m-%d").date()
  sequencing_run_fields = [f'run{run_id}',
                           f'himc{himc_pool}',
                           sequencing_date_object.strftime("%m%d%y")]
  return SEQUENCING_RUN_FIELD_DELIMITER.join(sequencing_run_fields)

def generate_experiment_name(sequencing_runs, **_):
  sequencing_run_names = map(generate_sequencing_run_name, sequencing_runs)
  return SEQUENCING_RUN_NAME_DELIMITER.join(sequencing_run_names)

def get_oligo_group(target_oligo_group_id, oligo_groups):
  try:
    return [og for og in oligo_groups if og['id'] == target_oligo_group_id][0]
  except IndexError:
    raise Exception(f"Oligo group with id {target_oligo_group_id} not found")

def submit_analysis(sample, experiment, oligo_groups, cellranger_version, depends_on = []):
  experiment_name = generate_experiment_name(**experiment)
  feature_barcoding_enabled = sample['feature_barcoding']['enabled']
  if feature_barcoding_enabled:
    target_oligo_group_id = sample['feature_barcoding']['oligo_group_id']
    oligo_group = get_oligo_group(target_oligo_group_id, oligo_groups)

  container_overrides = {
    "environment": [
      {
        "name": "DEBUG",
        "value": str(experiment.get('meta', {}).get('debug', False) is True).lower()
      },
      {
        "name": "POOLED",
        "value": str(sample['pooled_run'])
      }
    ]
  }
  job_configuration = {
    "experiment_name": experiment_name,
    "runs": [sequencing_run['id'] for sequencing_run in experiment['sequencing_runs']],
    "sample": sample,
    "oligo_group": (oligo_group if feature_barcoding_enabled else None),
  }
  parameters = {
    "command": "run_analysis",
    "configuration": json.dumps(job_configuration)
  }

  job_definition = f"{PIPELINE_BASE_NAME}-cellranger-{cellranger_version.replace('.', '_')}-bcl2fastq-2_20_0"

  try:
    response = batch_client.submit_job(
      containerOverrides=container_overrides,
      dependsOn=depends_on,
      jobDefinition=job_definition,
      jobName=f'{experiment_name}-{sample["job_type"]}-{sample["name"]}',
      jobQueue=JOB_QUEUE,
      parameters=parameters
    )
    # Log response from AWS Batch
    print("debug: " + json.dumps(response, indent=2))
    return response['jobId']
  except Exception as e:
    print(e)
    message = 'Error submitting Batch Job'
    print(message)
    raise Exception(message)

def submit_mkfastq(bcl_file, experiment, run_id, samples, cellranger_version):
  experiment_name = generate_experiment_name(**experiment)
  container_overrides = {
    "environment": [
      {
        "name": "DEBUG",
        "value": str(experiment.get('meta', {}).get('debug', False) is True).lower()
      }
    ]
  }
  job_configuration = {
    "bcl_file": bcl_file,
    "experiment_name": experiment_name,
    "run_id": run_id,
    "samples": samples
  }
  parameters = {
    "command": "run_mkfastq",
    "configuration": json.dumps(job_configuration)
  }


  job_definition = f"{PIPELINE_BASE_NAME}-cellranger-{cellranger_version.replace('.', '_')}-bcl2fastq-2_20_0"

  try:
    response = batch_client.submit_job(
      containerOverrides=container_overrides,
      jobDefinition=job_definition,
      jobName=f'{experiment_name}-mkfastq',
      jobQueue=JOB_QUEUE,
      parameters=parameters
    )
    # Log response from AWS Batch
    print("debug: " + json.dumps(response, indent=2))
    return response['jobId']
  except Exception as e:
    print(e)
    message = 'Error getting Batch Job status'
    print(message)
    raise Exception(message)

def submit_bcl2fastq(bcl_file, experiment, run_id, samples):
  experiment_name = generate_experiment_name(**experiment)
  container_overrides = {
    "environment": [
      {
        "name": "DEBUG",
        "value": str(experiment.get('meta', {}).get('debug', False) is True).lower()
      }
    ]
  }
  job_configuration = {
    "bcl_file": bcl_file,
    "experiment_name": experiment_name,
    "run_id": run_id,
    "samples": samples
  }
  parameters = {
    "command": "run_bcl2fastq",
    "configuration": json.dumps(job_configuration)
  }

  # We don't necessarily even need the cellranger software. We arbitrarily choose the latest version.
  job_definition = f"{PIPELINE_BASE_NAME}-cellranger-3_0_2-bcl2fastq-2_20_0"

  try:
    response = batch_client.submit_job(
      containerOverrides=container_overrides,
      jobDefinition=job_definition,
      jobName=f'{experiment_name}-bcl2fastq',
      jobQueue=JOB_QUEUE,
      parameters=parameters
    )
    # Log response from AWS Batch
    print("debug: " + json.dumps(response, indent=2))
    return response['jobId']
  except Exception as e:
    print(e)
    message = 'Error getting Batch Job status'
    print(message)
    raise Exception(message)

def upload_config(config_yaml_filename):
  configuration = load_config(config_yaml_filename)
  experiment = configuration['experiment']
  experiment_name = generate_experiment_name(**experiment)

  config_basename = os.path.basename(config_yaml_filename)

  # Our s3 bucket has versioning enabled. Because of this, we can overwrite 
  # pre-existing configuration files without worry.
  s3_client.upload_file(config_yaml_filename, S3_BUCKET, f'{experiment_name}/configs/{config_basename}')

def load_config(filename):
  with open(filename) as config_yaml:
    return yaml.load(config_yaml)

def validate_configuration(configuration):
  schema_path = os.path.realpath('scripts/schemas/config.json')
  schema = json.load(open(schema_path))

  resolver = RefResolver(f'file://{schema_path}', None)

  Draft4Validator.check_schema(schema)

  validator = Draft4Validator(schema, resolver=resolver, format_checker=None)
  validator.validate(configuration)

def main(config_yaml_filename):
  configuration = load_config(config_yaml_filename)

  validate_configuration(configuration)

  experiment = configuration['experiment']
  bcl2fastq_version = experiment['bcl2fastq_version']
  cellranger_version = experiment['cellranger_version']

  processing = configuration['processing']
  processing_job_ids = []

  upload_config(config_yaml_filename)

  if processing and 'mkfastq' in processing:
    samples = processing['mkfastq']['samples']
    for bcl_file, run_id in [[sequencing_run['bcl_file'], sequencing_run['id']] for sequencing_run in experiment['sequencing_runs']]:
      print(f'info: processing: mkfastq: submitting: {bcl_file}')
      mkfastq_job_id = submit_mkfastq(bcl_file=bcl_file,
                                      experiment=experiment,
                                      run_id=run_id,
                                      samples=samples,
                                      cellranger_version=cellranger_version)
      processing_job_ids.append(mkfastq_job_id)
      print(f'info: processing: mkfastq: submitted: {bcl_file}')

  if processing and 'bcl2fastq' in processing:
    samples = processing['bcl2fastq']['samples']
    for bcl_file, run_id in [[sequencing_run['bcl_file'], sequencing_run['id']] for sequencing_run in experiment['sequencing_runs']]:
      print(f'info: processing: bcl2fastq: submitting: {bcl_file}')
      bcl2fastq_job_id = submit_bcl2fastq(bcl_file=bcl_file,
                                          experiment=experiment,
                                          run_id=run_id,
                                          samples=samples)
      processing_job_ids.append(bcl2fastq_job_id)
      print(f'info: processing: bcl2fastq: submitted: {bcl_file}')

  if configuration['analyses']:
    for sample in configuration['analyses']['samples']:
      print(f'info: analyses: submitting: {sample["name"]}')
      oligo_groups = 'oligo_groups' in configuration['analyses'] and configuration['analyses']['oligo_groups']
      submit_analysis(sample,
                      experiment=experiment,
                      oligo_groups=oligo_groups,
                      cellranger_version=cellranger_version,
                      depends_on=[ {"jobId": job_id} for job_id in processing_job_ids ])
      print(f'info: analyses: submitted: {sample["name"]}')

  print("info: all jobs submitted successfully.")

if __name__ == "__main__":
  if len(sys.argv) < 2:
    usage()
    exit(1)

  main(sys.argv[1])
